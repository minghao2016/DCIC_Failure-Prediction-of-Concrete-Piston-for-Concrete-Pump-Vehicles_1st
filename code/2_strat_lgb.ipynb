{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import gc\n",
    "import datetime\n",
    "import itertools\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import lightgbm as lgb\n",
    "# import xgboost as xgb\n",
    "# import catboost\n",
    "from tqdm import *\n",
    "from scipy import stats\n",
    "import warnings\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.model_selection import KFold, RepeatedKFold,train_test_split,StratifiedKFold\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder,MinMaxScaler\n",
    "from collections import Counter, defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, f1_score, log_loss,roc_auc_score\n",
    "import seaborn as sns\n",
    "color = sns.color_palette()\n",
    "sns.set(style=\"whitegrid\")\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.1'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgb.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_variable(v,filename):\n",
    "    f=open(filename,'wb')\n",
    "    pickle.dump(v,f)\n",
    "    f.close()\n",
    "    return filename\n",
    " \n",
    "def load_variavle(filename):\n",
    "    f=open(filename,'rb')\n",
    "    r=pickle.load(f)\n",
    "    f.close()\n",
    "    return r\n",
    "\n",
    "def static_fe(data1,data2,df,column,values,cc,c):\n",
    "    addn = df[[column,values]].copy()\n",
    "    addn = addn.groupby(column)[values].agg(cc).reset_index()\n",
    "    addn.columns = [column] + [c+values+'_'+i for i in cc]\n",
    "    data1 = data1.merge(addn,on=column,how='left')\n",
    "    data2 = data2.merge(addn,on=column,how='left')\n",
    "    return data1,data2\n",
    "\n",
    "def cons(x):\n",
    "    num_times = [(k, len(list(v))) for k, v in itertools.groupby(list(x))]\n",
    "    num_times = pd.DataFrame(num_times)\n",
    "    num_times = num_times[num_times[0] == 1][1]\n",
    "    return num_times.max()\n",
    "\n",
    "def cons_fe(data,df,column,values):\n",
    "    kk = df.groupby(column)[values].apply(cons)\n",
    "    kk = kk.fillna(0).astype('int32').reset_index()\n",
    "    kk.columns = [column,'cons_' + values]\n",
    "    data = data.merge(kk, on=column, how='left')\n",
    "    return data\n",
    "\n",
    "def get_new_columns(name,aggs):\n",
    "    return [name + '_' + k + '_' + agg for k in aggs.keys() for agg in aggs[k]]\n",
    "\n",
    "def auc(y,pred):\n",
    "#     fpr, tpr, thresholds = metrics.roc_curve(y, pred)\n",
    "    return roc_auc_score(y, pred)\n",
    "\n",
    "def f1(y,pred):\n",
    "#     fpr, tpr, thresholds = metrics.roc_curve(y, pred)\n",
    "    return f1_score(y, pred,average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_map = load_variavle('../data/id_map.pkl')\n",
    "new_map = {v:k for k, v in id_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = ['活塞工作时长', '发动机转速', '油泵转速', '泵送压力', '液压油温', '流量档位', '分配压力', '排量电流',\n",
    "       '低压开关', '高压开关', '搅拌超压信号', '正泵', '反泵', '设备类型', 'sample_file_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all = load_variavle('../data/data_all.pkl')\n",
    "data_all['活塞工作时长'] = data_all['活塞工作时长'].replace(2098,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('../data/train_labels.csv')\n",
    "test_data = pd.read_csv('../data/submit_example.csv')\n",
    "train_data['sample_file_name'] = train_data['sample_file_name'].map(new_map)\n",
    "test_data['sample_file_name'] = test_data['sample_file_name'].map(new_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "kk = data_all.groupby(['设备类型','sample_file_name']).size().reset_index().rename(columns={0:'length'})\n",
    "train_data = train_data.merge(kk, on=['sample_file_name'], how='left')\n",
    "test_data = test_data.merge(kk, on=['sample_file_name'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_func = {\n",
    "#     \"\"\"0.6269\"\"\"\n",
    "            '发动机转速': ['median', 'max', 'min', 'std',],\n",
    "        '油泵转速': ['median', 'max', 'min', 'std',],\n",
    "        '活塞工作时长': ['mean', 'max', 'min'],\n",
    "        '泵送压力': ['median',  'max', 'min', 'std',],\n",
    "        '液压油温': ['median',  'max', 'min', 'std'],\n",
    "        '流量档位': ['median',  'max', 'min', 'std','sum'],\n",
    "        '分配压力': ['median',  'max', 'min', 'std','sum'],\n",
    "        '排量电流': ['median',  'max', 'min', 'std','sum'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_columns = get_new_columns('id',agg_func)\n",
    "df_group = data_all.groupby('sample_file_name').agg(agg_func)\n",
    "df_group.columns = new_columns\n",
    "df_group.reset_index(drop=False,inplace=True)\n",
    "train_data = train_data.merge(df_group, on='sample_file_name', how='left')\n",
    "test_data = test_data.merge(df_group, on='sample_file_name', how='left')\n",
    "del df_group;gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = [i for i in train_data.columns if i not in ['sample_file_name', 'label']]\n",
    "X_train = train_data[col]\n",
    "y_train = train_data['label'].astype(int)\n",
    "X_test = test_data[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train.fillna(0))\n",
    "X_train[X_train.columns] = scaler.transform(X_train.fillna(0))\n",
    "X_test[X_test.columns] = scaler.transform(X_test.fillna(0)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_f1_score(y_hat, data):\n",
    "    y_true = data.get_label()\n",
    "    y_hat = np.round(y_hat) # scikits f1 doesn't like probabilities\n",
    "    return 'f1', f1_score(y_true, y_hat, average='macro'), True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 5\n",
    "seed = 2019\n",
    "skf = StratifiedKFold(n_splits=K, shuffle=True, random_state=seed)\n",
    "lgb_params = {\n",
    "                        'boosting_type': 'gbdt',\n",
    "                        'objective': 'binary',\n",
    "                        'metric': 'binary_logloss',#auc\n",
    "                        'num_leaves': 2**7,#2**7+7\n",
    "                        'subsample': 0.8,#0.7,0.8\n",
    "                        'colsample_bytree': 0.7,#0.5,0.7\n",
    "                        'learning_rate': 0.01,#0.05\n",
    "                        'seed': 2017,#2017\n",
    "                        'nthread': 6,\n",
    "                        'silent': True\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[250]\ttraining's binary_logloss: 0.573437\ttraining's f1: 0.74627\tvalid_1's binary_logloss: 0.616356\tvalid_1's f1: 0.676255\n",
      "[500]\ttraining's binary_logloss: 0.522036\ttraining's f1: 0.794122\tvalid_1's binary_logloss: 0.602536\tvalid_1's f1: 0.6824\n",
      "[750]\ttraining's binary_logloss: 0.483385\ttraining's f1: 0.834137\tvalid_1's binary_logloss: 0.597312\tvalid_1's f1: 0.685102\n",
      "[1000]\ttraining's binary_logloss: 0.449666\ttraining's f1: 0.867431\tvalid_1's binary_logloss: 0.592479\tvalid_1's f1: 0.687886\n",
      "[1250]\ttraining's binary_logloss: 0.419551\ttraining's f1: 0.896375\tvalid_1's binary_logloss: 0.588463\tvalid_1's f1: 0.689196\n",
      "[1500]\ttraining's binary_logloss: 0.392175\ttraining's f1: 0.921475\tvalid_1's binary_logloss: 0.585017\tvalid_1's f1: 0.691418\n",
      "Early stopping, best iteration is:\n",
      "[1408]\ttraining's binary_logloss: 0.401957\ttraining's f1: 0.913039\tvalid_1's binary_logloss: 0.58649\tvalid_1's f1: 0.6923\n",
      "best iteration =  1408\n",
      "fold 1\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[250]\ttraining's binary_logloss: 0.572963\ttraining's f1: 0.742508\tvalid_1's binary_logloss: 0.616454\tvalid_1's f1: 0.673434\n",
      "[500]\ttraining's binary_logloss: 0.523359\ttraining's f1: 0.790798\tvalid_1's binary_logloss: 0.604663\tvalid_1's f1: 0.678202\n",
      "[750]\ttraining's binary_logloss: 0.48495\ttraining's f1: 0.832244\tvalid_1's binary_logloss: 0.599103\tvalid_1's f1: 0.683364\n",
      "[1000]\ttraining's binary_logloss: 0.451684\ttraining's f1: 0.866933\tvalid_1's binary_logloss: 0.595121\tvalid_1's f1: 0.686833\n",
      "[1250]\ttraining's binary_logloss: 0.42144\ttraining's f1: 0.896679\tvalid_1's binary_logloss: 0.591254\tvalid_1's f1: 0.691171\n",
      "[1500]\ttraining's binary_logloss: 0.394318\ttraining's f1: 0.921169\tvalid_1's binary_logloss: 0.588152\tvalid_1's f1: 0.692742\n",
      "Early stopping, best iteration is:\n",
      "[1586]\ttraining's binary_logloss: 0.385753\ttraining's f1: 0.927661\tvalid_1's binary_logloss: 0.587368\tvalid_1's f1: 0.693862\n",
      "best iteration =  1586\n",
      "fold 2\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[250]\ttraining's binary_logloss: 0.573722\ttraining's f1: 0.745981\tvalid_1's binary_logloss: 0.611833\tvalid_1's f1: 0.681928\n",
      "[500]\ttraining's binary_logloss: 0.522854\ttraining's f1: 0.795671\tvalid_1's binary_logloss: 0.599948\tvalid_1's f1: 0.687769\n",
      "[750]\ttraining's binary_logloss: 0.485574\ttraining's f1: 0.835054\tvalid_1's binary_logloss: 0.594907\tvalid_1's f1: 0.689986\n",
      "Early stopping, best iteration is:\n",
      "[793]\ttraining's binary_logloss: 0.479535\ttraining's f1: 0.841018\tvalid_1's binary_logloss: 0.594173\tvalid_1's f1: 0.692724\n",
      "best iteration =  793\n",
      "fold 3\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[250]\ttraining's binary_logloss: 0.573315\ttraining's f1: 0.745821\tvalid_1's binary_logloss: 0.614004\tvalid_1's f1: 0.677207\n",
      "[500]\ttraining's binary_logloss: 0.522581\ttraining's f1: 0.792507\tvalid_1's binary_logloss: 0.600937\tvalid_1's f1: 0.686012\n",
      "Early stopping, best iteration is:\n",
      "[631]\ttraining's binary_logloss: 0.502252\ttraining's f1: 0.812918\tvalid_1's binary_logloss: 0.598166\tvalid_1's f1: 0.688273\n",
      "best iteration =  631\n",
      "fold 4\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[250]\ttraining's binary_logloss: 0.572543\ttraining's f1: 0.74828\tvalid_1's binary_logloss: 0.616931\tvalid_1's f1: 0.672319\n",
      "[500]\ttraining's binary_logloss: 0.522217\ttraining's f1: 0.794317\tvalid_1's binary_logloss: 0.606029\tvalid_1's f1: 0.680003\n",
      "[750]\ttraining's binary_logloss: 0.483849\ttraining's f1: 0.836151\tvalid_1's binary_logloss: 0.601212\tvalid_1's f1: 0.682105\n",
      "[1000]\ttraining's binary_logloss: 0.44974\ttraining's f1: 0.87009\tvalid_1's binary_logloss: 0.597573\tvalid_1's f1: 0.685566\n",
      "Early stopping, best iteration is:\n",
      "[1079]\ttraining's binary_logloss: 0.439887\ttraining's f1: 0.880007\tvalid_1's binary_logloss: 0.596572\tvalid_1's f1: 0.687114\n",
      "best iteration =  1079\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6908631209696624"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "oof = np.zeros(len(X_train))\n",
    "predictions = np.zeros(len(X_test))\n",
    "feature_importance_df = pd.DataFrame()\n",
    "\n",
    "for i, (train_index, val_index) in enumerate(skf.split(X_train,y_train)):\n",
    "    print(\"fold {}\".format(i))\n",
    "    X_tr, X_val = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "    y_tr, y_val = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "    \n",
    "    lgb_train = lgb.Dataset(X_tr,y_tr)\n",
    "    lgb_val = lgb.Dataset(X_val,y_val)\n",
    "    num_round = 3000\n",
    "    clf = lgb.train(lgb_params, lgb_train, num_round, valid_sets = [lgb_train, lgb_val],\n",
    "                    feval=lgb_f1_score, verbose_eval=250, early_stopping_rounds = 100)\n",
    "    oof[val_index] = clf.predict(X_val, num_iteration=clf.best_iteration)\n",
    "    print('best iteration = ',clf.best_iteration)\n",
    "    fold_importance_df = pd.DataFrame()\n",
    "    fold_importance_df[\"Feature\"] = clf.feature_name()\n",
    "    fold_importance_df[\"importance\"] = clf.feature_importance()\n",
    "    fold_importance_df[\"fold\"] = i + 1\n",
    "    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "    predictions += clf.predict(X_test, num_iteration=clf.best_iteration) / skf.n_splits\n",
    "f1_score(y_train, np.round(oof), average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0    34005\n",
      "1.0    18245\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "sub = test_data[['sample_file_name']].copy()\n",
    "sub['label'] = predictions\n",
    "sub['sample_file_name'] = sub['sample_file_name'].map(id_map)\n",
    "print(sub['label'].round().value_counts())\n",
    "sub = sub.sort_values('label', ascending=False).reset_index(drop=True)\n",
    "sub.loc[:25000, 'label'] = 1\n",
    "sub.loc[25000:, 'label'] = 0\n",
    "sub['label'] = sub['label'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    27250\n",
      "1    25000\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(sub['label'].value_counts())\n",
    "sub.to_csv('../sub/lgb.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
