{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import gc\n",
    "import datetime\n",
    "import itertools\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier\n",
    "# from sklearn.neighbors import KNeighborsClassifier \n",
    "# from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier,ExtraTreesClassifier\n",
    "from tqdm import *\n",
    "from scipy import stats\n",
    "import warnings\n",
    "from sklearn.model_selection import KFold, RepeatedKFold,train_test_split,StratifiedKFold\n",
    "# from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from collections import Counter, defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler ,StandardScaler\n",
    "from sklearn.metrics import f1_score, log_loss, roc_auc_score, accuracy_score\n",
    "import seaborn as sns\n",
    "color = sns.color_palette()\n",
    "sns.set(style=\"whitegrid\")\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_variable(v,filename):\n",
    "    f=open(filename,'wb')\n",
    "    pickle.dump(v,f)\n",
    "    f.close()\n",
    "    return filename\n",
    " \n",
    "def load_variavle(filename):\n",
    "    f=open(filename,'rb')\n",
    "    r=pickle.load(f)\n",
    "    f.close()\n",
    "    return r\n",
    "\n",
    "def static_fe(data1,data2,df,column,values,cc,c):\n",
    "    addn = df[[column,values]].copy()\n",
    "    addn = addn.groupby(column)[values].agg(cc).reset_index()\n",
    "    addn.columns = [column] + [c+values+'_'+i for i in cc]\n",
    "    data1 = data1.merge(addn,on=column,how='left')\n",
    "    data2 = data2.merge(addn,on=column,how='left')\n",
    "    return data1,data2\n",
    "\n",
    "def cons(x):\n",
    "    num_times = [(k, len(list(v))) for k, v in itertools.groupby(list(x))]\n",
    "    num_times = pd.DataFrame(num_times)\n",
    "    num_times = num_times[num_times[0] == 1][1]\n",
    "    return num_times.max()\n",
    "\n",
    "def cons_fe(data,df,column,values):\n",
    "    kk = df.groupby(column)[values].apply(cons)\n",
    "    kk = kk.fillna(0).astype('int32').reset_index()\n",
    "    kk.columns = [column,'cons_' + values]\n",
    "    data = data.merge(kk, on=column, how='left')\n",
    "    return data\n",
    "\n",
    "def get_new_columns(name,aggs):\n",
    "    return [name + '_' + k + '_' + agg for k in aggs.keys() for agg in aggs[k]]\n",
    "\n",
    "def auc(y,pred):\n",
    "#     fpr, tpr, thresholds = metrics.roc_curve(y, pred)\n",
    "    return roc_auc_score(y, pred)\n",
    "\n",
    "def f1(y,pred):\n",
    "#     fpr, tpr, thresholds = metrics.roc_curve(y, pred)\n",
    "    return f1_score(y, pred,average='macro')\n",
    "def lgb_f1_score(y_hat, data):\n",
    "    y_true = data.get_label()\n",
    "#     y_hat[y_hat>0.45] = 1\n",
    "#     y_hat[y_hat<=0.45] = 0\n",
    "    y_hat = np.round(y_hat) # scikits f1 doesn't like probabilities\n",
    "    return 'f1', f1_score(y_true, y_hat, average='macro'), True\n",
    "def lgb_accuracy_score(y_hat, data):\n",
    "    y_true = data.get_label()\n",
    "    y_hat = np.round(y_hat)\n",
    "    return 'accuracy', accuracy_score(y_true, y_hat), True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_map = load_variavle('../data/id_map.pkl')\n",
    "new_map = {v:k for k, v in id_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = ['活塞工作时长', '发动机转速', '油泵转速', '泵送压力', '液压油温', '流量档位', '分配压力', '排量电流',\n",
    "       '低压开关', '高压开关', '搅拌超压信号', '正泵', '反泵', '设备类型', 'sample_file_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all = load_variavle('../data/data_all.pkl')\n",
    "data_all['活塞工作时长'] = data_all['活塞工作时长'].replace(2098,1)\n",
    "\"\"\"0.628\"\"\"\n",
    "shebeileixing = {7: 573, 6: 44, 5: 78, 4: 63, 3: 9, 2: 4, 1: 252}\n",
    "data_all['设备类型'] = data_all['设备类型'].map(shebeileixing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('../data/train_labels.csv')\n",
    "test_data = pd.read_csv('../data/submit_example.csv')\n",
    "train_data['sample_file_name'] = train_data['sample_file_name'].map(new_map)\n",
    "test_data['sample_file_name'] = test_data['sample_file_name'].map(new_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(63817, 4) (52250, 4)\n"
     ]
    }
   ],
   "source": [
    "kk = data_all.groupby(['设备类型','sample_file_name']).size().reset_index().rename(columns={0:'length'})\n",
    "train_data = train_data.merge(kk, on=['sample_file_name'], how='left')\n",
    "test_data = test_data.merge(kk, on=['sample_file_name'], how='left')\n",
    "print(train_data.shape, test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_func = {\n",
    "#     \"\"\"0.6269\"\"\"\n",
    "            '发动机转速': ['median', 'mean', 'max', 'min', 'var'],\n",
    "        '油泵转速': ['median', 'mean', 'max', 'min', 'var'],\n",
    "        '活塞工作时长': ['mean', 'max', 'min'],\n",
    "        '泵送压力': ['median', 'mean', 'max', 'min', 'var'],\n",
    "        '液压油温': ['median', 'mean', 'max', 'min', 'var'],\n",
    "        '流量档位': ['median', 'mean', 'max', 'min', 'var','sum'],\n",
    "        '分配压力': ['median', 'mean', 'max', 'min', 'var','sum'],\n",
    "        '排量电流': ['median', 'mean', 'max', 'min', 'var','sum'],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_columns = get_new_columns('id',agg_func)\n",
    "df_group = data_all.groupby('sample_file_name').agg(agg_func)\n",
    "df_group.columns = new_columns\n",
    "df_group.reset_index(drop=False,inplace=True)\n",
    "train_data = train_data.merge(df_group, on='sample_file_name', how='left')\n",
    "test_data = test_data.merge(df_group, on='sample_file_name', how='left')\n",
    "del df_group;gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_data[train_data['设备类型'] == 573].reset_index(drop=True)\n",
    "test_df = test_data[test_data['设备类型'] == 573].reset_index(drop=True)\n",
    "del train_df['设备类型']\n",
    "del test_df['设备类型']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38975, 42) (32020, 42)\n"
     ]
    }
   ],
   "source": [
    "col = [i for i in train_df.columns if i not in ['sample_file_name', 'label']]\n",
    "X_train = train_df[col]\n",
    "y_train = train_df['label'].astype(int)\n",
    "X_test = test_df[col]\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train.fillna(0))\n",
    "X_train[X_train.columns] = scaler.transform(X_train.fillna(0))\n",
    "X_test[X_test.columns] = scaler.transform(X_test.fillna(0)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 5\n",
    "seed = 2019#2019\n",
    "skf = StratifiedKFold(n_splits=K, shuffle=True, random_state=seed)\n",
    "lgb_params = {\n",
    "                        'boosting_type': 'gbdt',\n",
    "                        'objective': 'binary',\n",
    "                        'metric': 'auc',#binary_logloss\n",
    "                        'num_leaves': 2**7,#2**7+7\n",
    "                        'subsample': 0.7,#0.7\n",
    "                        'colsample_bytree': 0.5,#0.5\n",
    "                        'learning_rate': 0.01,#0.05\n",
    "                        'seed': 2017,#2017\n",
    "                        'nthread': 6,\n",
    "                        'silent': True\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[250]\ttraining's auc: 0.867833\ttraining's f1: 0.791264\tvalid_1's auc: 0.745934\tvalid_1's f1: 0.6906\n",
      "[500]\ttraining's auc: 0.937951\ttraining's f1: 0.854672\tvalid_1's auc: 0.755642\tvalid_1's f1: 0.699411\n",
      "[750]\ttraining's auc: 0.971418\ttraining's f1: 0.901618\tvalid_1's auc: 0.762198\tvalid_1's f1: 0.70246\n",
      "Early stopping, best iteration is:\n",
      "[712]\ttraining's auc: 0.967804\ttraining's f1: 0.895366\tvalid_1's auc: 0.761235\tvalid_1's f1: 0.70357\n",
      "best iteration =  712\n",
      "fold 1\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[250]\ttraining's auc: 0.867803\ttraining's f1: 0.793616\tvalid_1's auc: 0.742058\tvalid_1's f1: 0.686322\n",
      "[500]\ttraining's auc: 0.93724\ttraining's f1: 0.857174\tvalid_1's auc: 0.756276\tvalid_1's f1: 0.699623\n",
      "[750]\ttraining's auc: 0.970481\ttraining's f1: 0.900142\tvalid_1's auc: 0.764165\tvalid_1's f1: 0.703964\n",
      "Early stopping, best iteration is:\n",
      "[726]\ttraining's auc: 0.968276\ttraining's f1: 0.896835\tvalid_1's auc: 0.76387\tvalid_1's f1: 0.705034\n",
      "best iteration =  726\n",
      "fold 2\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[250]\ttraining's auc: 0.865485\ttraining's f1: 0.792657\tvalid_1's auc: 0.730177\tvalid_1's f1: 0.675074\n",
      "[500]\ttraining's auc: 0.933969\ttraining's f1: 0.853565\tvalid_1's auc: 0.743823\tvalid_1's f1: 0.683964\n",
      "[750]\ttraining's auc: 0.96838\ttraining's f1: 0.897214\tvalid_1's auc: 0.749907\tvalid_1's f1: 0.688201\n",
      "Early stopping, best iteration is:\n",
      "[715]\ttraining's auc: 0.964976\ttraining's f1: 0.892317\tvalid_1's auc: 0.749256\tvalid_1's f1: 0.689615\n",
      "best iteration =  715\n",
      "fold 3\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[250]\ttraining's auc: 0.865849\ttraining's f1: 0.790095\tvalid_1's auc: 0.735176\tvalid_1's f1: 0.673943\n",
      "[500]\ttraining's auc: 0.935464\ttraining's f1: 0.853422\tvalid_1's auc: 0.748884\tvalid_1's f1: 0.687834\n",
      "[750]\ttraining's auc: 0.969061\ttraining's f1: 0.899365\tvalid_1's auc: 0.756379\tvalid_1's f1: 0.696328\n",
      "[1000]\ttraining's auc: 0.985836\ttraining's f1: 0.93132\tvalid_1's auc: 0.760168\tvalid_1's f1: 0.698173\n",
      "Early stopping, best iteration is:\n",
      "[1020]\ttraining's auc: 0.986722\ttraining's f1: 0.933421\tvalid_1's auc: 0.760498\tvalid_1's f1: 0.700853\n",
      "best iteration =  1020\n",
      "fold 4\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[250]\ttraining's auc: 0.868039\ttraining's f1: 0.798019\tvalid_1's auc: 0.744141\tvalid_1's f1: 0.686779\n",
      "[500]\ttraining's auc: 0.935967\ttraining's f1: 0.858124\tvalid_1's auc: 0.756963\tvalid_1's f1: 0.694254\n",
      "[750]\ttraining's auc: 0.970142\ttraining's f1: 0.901607\tvalid_1's auc: 0.762877\tvalid_1's f1: 0.699714\n",
      "Early stopping, best iteration is:\n",
      "[814]\ttraining's auc: 0.975684\ttraining's f1: 0.910732\tvalid_1's auc: 0.763864\tvalid_1's f1: 0.701558\n",
      "best iteration =  814\n",
      "macro f1 score :  0.7001303460166408\n"
     ]
    }
   ],
   "source": [
    "oof = np.zeros(len(X_train))\n",
    "predictions = np.zeros(len(X_test))\n",
    "feature_importance_df = pd.DataFrame()\n",
    "\n",
    "for i, (train_index, val_index) in enumerate(skf.split(X_train,y_train)):\n",
    "    print(\"fold {}\".format(i))\n",
    "    X_tr, X_val = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "    y_tr, y_val = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "    \n",
    "    lgb_train = lgb.Dataset(X_tr,y_tr)\n",
    "    lgb_val = lgb.Dataset(X_val,y_val)\n",
    "    num_round = 3000#1250\n",
    "    clf = lgb.train(lgb_params, lgb_train, num_round, valid_sets = [lgb_train, lgb_val], \n",
    "                    feval=lgb_f1_score,\n",
    "                    verbose_eval=250, early_stopping_rounds = 100)\n",
    "    oof[val_index] = clf.predict(X_val, num_iteration=clf.best_iteration)\n",
    "    print('best iteration = ',clf.best_iteration)\n",
    "    fold_importance_df = pd.DataFrame()\n",
    "    fold_importance_df[\"Feature\"] = clf.feature_name()\n",
    "    fold_importance_df[\"importance\"] = clf.feature_importance()\n",
    "    fold_importance_df[\"fold\"] = i + 1\n",
    "    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "    \n",
    "    predictions += clf.predict(X_test, num_iteration=clf.best_iteration) / skf.n_splits\n",
    "print('macro f1 score : ', f1_score(y_train, np.round(oof), average='macro'))\n",
    "#auc: 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0    20387\n",
      "1.0    11633\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "sub = test_df[['sample_file_name']].copy()\n",
    "sub['label'] = predictions\n",
    "sub['sample_file_name'] = sub['sample_file_name'].map(id_map)\n",
    "sub = sub.sort_values('label', ascending=False).reset_index(drop=True)\n",
    "print(sub['label'].round().value_counts())\n",
    "sub['label'] = sub['label'].round().astype(int)\n",
    "#11924,11629,11581,最新11633"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv('../sub/houchuli.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cc = pd.read_csv('../sub/best.csv')\n",
    "\n",
    "# dd = sub[sub['label'] == 1]['sample_file_name'].values\n",
    "\n",
    "# cc.loc[cc['sample_file_name'].isin(dd), 'label'] = 1\n",
    "\n",
    "# cc.label.value_counts()\n",
    "\n",
    "# cc.to_csv('../sub/sub_fuck3_23.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
